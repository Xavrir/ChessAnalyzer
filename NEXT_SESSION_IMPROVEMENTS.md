# Next Session: Accuracy & Opening Detection Improvements

## Date: October 11, 2025

## What We Just Completed ‚úÖ

### 1. Opening Detection System - FULLY IMPLEMENTED ‚úÖ
- **Created**: `lib/chess/openings.ts` - Comprehensive opening database with 60+ openings
- **Features**:
  - ECO code classification (A00-E99)
  - Detects King's Pawn, Queen's Pawn, English, Reti, and other openings
  - Variation detection (e.g., "Italian Game: Giuoco Piano")
  - Progressive matching (finds most specific opening variant)
- **Integration**: 
  - Added `opening` field to `GameAnalysis` interface in `store/useGameStore.ts`
  - Updated `hooks/useAnalysis.ts` to detect opening during analysis
  - Created `components/organisms/OpeningInfo.tsx` display component
  - Added opening display to `app/analyze/page.tsx` (shows after Advantage Chart)

### 2. Accuracy Calculation Verification ‚úÖ
- **Created**: `test-accuracy.ts` - Comprehensive test suite
- **Results**: ALL 6 TESTS PASS ‚úÖ
  - Perfect move (0cp loss): 100% ‚úÖ
  - Small loss (5cp): 50% ‚úÖ
  - Medium loss (10cp): 33.3% ‚úÖ
  - Large loss (100cp): 4.8% ‚úÖ
  - Black perspective: Correct ‚úÖ
  - Full game calculation: Working ‚úÖ

### 3. Identified Issue: Accuracy Percentages May Be Inflated üîç

**Current Implementation**:
```typescript
const accuracy = calculateGameAccuracy(
  moveHistory.map(m => ({ from: m.from, to: m.to })),
  evaluations,
  evaluations // ISSUE: Using same array for both actual and best evals
);
```

**The Problem**:
- `evaluations[i]` = Engine eval of position `i` (assumes best continuation)
- For move accuracy at position `i`:
  - `evalBefore` = `evaluations[i]`
  - `evalAfter` = `evaluations[i+1]` (actual move played)
  - `bestEval` = `evaluations[i]` (what SHOULD happen with best move)

**Why This Is Wrong**:
- `bestEval` should be the evaluation AFTER playing the best move from position `i`
- We're using the evaluation BEFORE the move instead
- This means we're comparing the position's static eval to itself

**Example**:
```
Position 0: Engine says eval = +20, best move = d5
Move played: e5 (suboptimal)
Position 1: Engine says eval = +30

Current calculation:
  evalBefore = +20
  evalAfter = +30  
  bestEval = +20 (WRONG! Should be eval after d5, not before)
  
Correct calculation:
  evalBefore = +20
  evalAfter = +30
  bestEval = +15 (what eval would be after d5)
  Loss = 15cp, not 0cp
```

**Why Percentages Seem High**:
- Fischer vs Spassky (GM-level game): White 71.2%, Black 72.8%
- These should probably be higher (85-95%) for a World Championship game
- The formula is correct, but input data is flawed

## What Needs To Be Done Next üéØ

### PRIORITY 1: Fix Accuracy Calculation üî¥

**Two Approaches**:

#### Option A: Re-analyze Best Moves (More Accurate, More Expensive)
```typescript
// After analyzing each position, also analyze the best move
for (let i = 0; i < positionHistory.length - 1; i++) {
  const fen = positionHistory[i];
  
  // 1. Analyze current position
  analyzePosition(fen);
  await waitForDepth20();
  const currentEval = getEval();
  const bestMove = getBestMove();
  
  // 2. Apply best move and analyze resulting position
  const bestMoveFen = applyMove(fen, bestMove);
  analyzePosition(bestMoveFen);
  await waitForDepth20();
  const bestEval = getEval();
  
  evaluations.push(currentEval);
  bestEvals.push(bestEval); // Now we have the TRUE best eval
}
```

**Pros**: Accurate, matches Chess.com methodology
**Cons**: Doubles analysis time (2x positions to analyze)

#### Option B: Use Engine's PV Evaluation (Faster, Good Approximation)
```typescript
// When engine returns info, it includes principal variation (PV)
// The eval in the PV assumes the best move is played
// So we can extract the eval from the PV

// In engine-worker.ts or uci-parser.ts, capture PV eval:
if (info.score && info.pv && info.pv.length > 0) {
  // The score already represents the evaluation AFTER the best move
  // because Stockfish evaluates the position after the PV
  bestEvals.push(info.score.cp);
}
```

**Pros**: No extra analysis needed, uses existing data
**Cons**: Slightly less accurate than re-analyzing

#### Option C: Heuristic Approximation (Fastest, Rough Estimate)
```typescript
// Use the current eval, but adjust based on eval change
// Assumption: If eval improved, best move would improve more
// If eval worsened, best move would maintain position

const bestEvals: number[] = [];
for (let i = 0; i < evaluations.length - 1; i++) {
  const current = evaluations[i];
  const next = evaluations[i + 1];
  const change = next - current;
  
  // If player perspective improved, best move would do even better
  // If player perspective worsened, best move would maintain
  const isWhite = i % 2 === 0;
  const playerChange = isWhite ? change : -change;
  
  if (playerChange >= 0) {
    // Move was good, best move would be same or better
    bestEvals.push(Math.max(current, next));
  } else {
    // Move was bad, best move would maintain position
    bestEvals.push(current);
  }
}
```

**Pros**: Instant, no extra computation
**Cons**: Very rough estimate, may still be inflated

### PRIORITY 2: Test Opening Detection üü°

**To Do**:
1. Start dev server: `npm run dev:all`
2. Clear port 3001 if needed: `fuser -k 3001/tcp`
3. Navigate to http://localhost:3002/analyze
4. Import Fischer vs Spassky 1972 Game 6 PGN
5. Click "ANALYZE GAME"
6. Wait for analysis to complete
7. **Check new Opening Info panel** (should appear after Advantage Chart)
8. **Expected**: Should show opening name, variation, and ECO code

**Test Cases**:
- Fischer-Spassky Game 6: Should detect "Queen's Gambit Declined" or similar
- Sicilian Defense games
- Italian Game
- Ruy Lopez
- King's Indian Defense

### PRIORITY 3: Verify Accuracy Percentages üü°

**After fixing accuracy calculation**:

1. Re-analyze Fischer vs Spassky game
2. Expected results for World Championship GM game:
   - White accuracy: 85-95%
   - Black accuracy: 85-95%
   - Few or no mistakes/blunders
   - Mostly "excellent" and "theory" moves

3. Test with games of different skill levels:
   - Beginner game: 30-50% accuracy
   - Intermediate: 60-75%
   - Advanced: 75-85%
   - GM: 85-95+%

## Files Modified in This Session üìù

1. **lib/chess/openings.ts** - NEW FILE
   - Opening database with 60+ openings
   - Detection algorithm
   - Display name formatting

2. **store/useGameStore.ts**
   - Added `opening?: OpeningInfo | null` to `GameAnalysis` interface
   - Added import for `OpeningInfo` type

3. **hooks/useAnalysis.ts**
   - Added opening detection after accuracy calculation
   - Imports: `detectOpening`, `extractSANMoves`
   - Stores opening in analysis results

4. **components/organisms/OpeningInfo.tsx** - NEW FILE
   - Display component for opening information
   - Shows ECO code, opening name, variation
   - Fallback for unknown openings

5. **app/analyze/page.tsx**
   - Added `OpeningInfoDisplay` import
   - Added opening panel after Advantage Chart
   - Displays when analysis is available

6. **test-accuracy.ts** - NEW FILE (for testing only)
   - Comprehensive test suite for accuracy calculations
   - All tests passing ‚úÖ

## Current State üìä

### Working Features ‚úÖ
- Opening detection fully implemented
- Opening display in UI
- Accuracy formula mathematically correct
- Annotation system working (no false blunders)
- UI redesign complete (wintrchess.com style)
- Real Stockfish 16.1 integration

### Known Issues ‚ö†Ô∏è
- **Accuracy percentages may be inflated** due to using same array for actual and best evaluations
- Need to implement proper "best eval" calculation (see Priority 1 above)

### Next Steps Summary üéØ
1. **Fix accuracy calculation** (Choose Option A, B, or C)
2. **Test opening detection** with various games
3. **Verify accuracy percentages** are realistic
4. Consider adding more openings to database if needed

## Testing Commands üß™

```bash
# Start development server
cd /root/Downloads/tes11/chess-analyzer
npm run dev:all

# Clear port 3001 if needed
fuser -k 3001/tcp

# Run accuracy tests
npx tsx test-accuracy.ts

# Build for production
npm run build
```

## Sample PGN for Testing üìã

**Fischer vs Spassky 1972 Game 6** (14 moves):
```
[Event "World Championship 1972"]
[Site "Reykjavik ISL"]
[Date "1972.07.23"]
[Round "6"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1-0"]

1.c4 e6 2.Nf3 d5 3.d4 Nf6 4.Nc3 Be7 5.Bg5 O-O 6.e3 h6 7.Bh4 b6 8.cxd5 Nxd5 9.Bxe7 Qxe7 10.Nxd5 exd5 11.Rc1 Be6 12.Qa4 c5 13.Qa3 Rc8 14.Bb5 1-0
```

## Recommendation for Next Session üí°

**Start with Option B (Use Engine's PV Evaluation)** because:
1. Faster than re-analyzing (Option A)
2. More accurate than heuristics (Option C)
3. Data already available from Stockfish
4. Good balance of accuracy and performance

**Implementation Plan**:
1. Modify `engine-worker.ts` to capture PV evaluation separately
2. Update `useAnalysis.ts` to use PV evals as "best evals"
3. Test with Fischer-Spassky game
4. Compare results before/after fix

Good luck! üéâ
